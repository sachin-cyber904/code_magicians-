import streamlit as st
import numpy as np
import cv2
from PIL import Image
import pandas as pd
import io
import json

# ---------- Helper functions ----------

def read_answer_key(file) -> dict:
    """Accepts CSV or JSON uploaded file and returns a dict {q: 'A'}."""
    try:
        content = file.read()
        file.seek(0)
        fname = file.name.lower()
        if fname.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(content))
            if 'question' in df.columns and 'answer' in df.columns:
                return {int(r['question']): str(r['answer']).strip().upper() for _, r in df.iterrows()}
            else:
                answers = [str(x).strip().upper() for x in df.iloc[:,0].tolist()]
                return {i+1: answers[i] for i in range(len(answers))}
        elif fname.endswith('.xlsx') or fname.endswith('.xls'):
            df = pd.read_excel(io.BytesIO(content))
            if 'question' in df.columns and 'answer' in df.columns:
                return {int(r['question']): str(r['answer']).strip().upper() for _, r in df.iterrows()}
            else:
                answers = [str(x).strip().upper() for x in df.iloc[:,0].tolist()]
                return {i+1: answers[i] for i in range(len(answers))}
        else:
            j = json.loads(content.decode('utf-8'))
            if isinstance(j, dict):
                return {int(k): str(v).strip().upper() for k,v in j.items()}
            elif isinstance(j, list):
                return {i+1: str(j[i]).strip().upper() for i in range(len(j))}
    except Exception:
        st.error('Unable to parse answer key file. Please upload CSV, Excel (question,answer) or JSON.')
        return {}

def resize(image, width=None, height=None):
    (h, w) = image.shape[:2]
    if width is None and height is None:
        return image
    if width is None:
        ratio = height / float(h)
        dim = (int(w * ratio), height)
    else:
        ratio = width / float(w)
        dim = (width, int(h * ratio))
    return cv2.resize(image, dim, interpolation=cv2.INTER_AREA)

def find_paper_contour(gray):
    """Find the largest 4-point contour that likely represents the sheet."""
    blurred = cv2.GaussianBlur(gray, (5,5), 0)
    edged = cv2.Canny(blurred, 75, 200)
    contours, _ = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]
    for c in contours:
        peri = cv2.arcLength(c, True)
        approx = cv2.approxPolyDP(c, 0.02 * peri, True)
        if len(approx) == 4:
            return approx.reshape(4,2)
    return None

def order_points(pts):
    rect = np.zeros((4,2), dtype='float32')
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect

def four_point_transform(image, pts):
    rect = order_points(pts)
    (tl, tr, br, bl) = rect
    widthA = np.linalg.norm(br - bl)
    widthB = np.linalg.norm(tr - tl)
    maxWidth = max(int(widthA), int(widthB))
    heightA = np.linalg.norm(tr - br)
    heightB = np.linalg.norm(tl - bl)
    maxHeight = max(int(heightA), int(heightB))
    dst = np.array([[0,0],[maxWidth-1,0],[maxWidth-1,maxHeight-1],[0,maxHeight-1]], dtype='float32')
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
    return warped

def detect_marked_bubbles(image, x, y, w, h, threshold=0.4):
    """Helper function to detect if a bubble is marked with adjustable threshold."""
    roi = image[y:y+h, x:x+w]
    total_pixels = roi.shape[0] * roi.shape[1]
    filled_pixels = cv2.countNonZero(roi)
    return filled_pixels / total_pixels > threshold

def grade_sheet(warped_gray, answer_key, questions=100, choices=4, subjects=5, layout=None):
    """
    Grid-based OMR detection optimized for template with:
    - 5 subjects side by side
    - 20 questions per subject vertically
    - A,B,C,D choices for each question
    """
    # Enhanced preprocessing for better bubble detection
    # Denoise with bilateral filter and apply contrast adjustment
    denoised = cv2.bilateralFilter(warped_gray, 9, 75, 75)
    # Adjust contrast to make filled bubbles stand out
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    contrasted = clahe.apply(denoised)
    # Adaptive threshold for varying lighting conditions
    thresh = cv2.adaptiveThreshold(contrasted, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                  cv2.THRESH_BINARY_INV, 11, 2)
    
    # Create debug image
    debug_img = cv2.cvtColor(warped_gray.copy(), cv2.COLOR_GRAY2BGR)
    
    # Get image dimensions
    h, w = thresh.shape
    
    # Calculate grid parameters (adjusted for header and footer space)
    header_offset = int(h * 0.15)  # Reduced header offset to 15%
    footer_offset = int(h * 0.05)  # Add small footer offset
    usable_h = h - header_offset - footer_offset
    questions_per_subject = questions // subjects
    subject_width = w // subjects
    row_height = usable_h // questions_per_subject
    choice_width = subject_width // choices
    
    results = {}
    
    # Process each subject
    for subject in range(subjects):
        subject_x = subject * subject_width
        
        # Process each question in this subject
        for q in range(questions_per_subject):
            question_y = header_offset + (q * row_height)
            marked = None
            max_confidence = 0
            
            # Check each choice (A,B,C,D)
            for choice in range(choices):
                x = subject_x + (choice * choice_width)
                y = question_y
                
                # Define bubble region with tighter padding
                bubble_w = int(choice_width * 0.5)  # Reduced to 50% for precision
                bubble_h = int(row_height * 0.5)
                bubble_x = x + (choice_width - bubble_w) // 2
                bubble_y = y + (row_height - bubble_h) // 2
                
                # Ensure bounds
                bubble_x = max(0, min(bubble_x, w - bubble_w))
                bubble_y = max(0, min(bubble_y, h - bubble_h))
                
                # Check if bubble is marked with adjustable threshold
                confidence = detect_marked_bubbles(thresh, bubble_x, bubble_y, bubble_w, bubble_h, threshold=0.35)
                
                # Draw rectangle for visualization (green if marked, orange if not)
                color = (0, 255, 0) if confidence else (0, 165, 255)
                cv2.rectangle(debug_img, 
                            (bubble_x, bubble_y), 
                            (bubble_x + bubble_w, bubble_y + bubble_h),
                            color, 2)
                cv2.putText(debug_img, chr(ord('A') + choice), (bubble_x, bubble_y - 5), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                if confidence and confidence > max_confidence:
                    max_confidence = confidence
                    marked = chr(ord('A') + choice)
            
            # Store result for this question
            q_num = subject * questions_per_subject + (q + 1)
            results[q_num] = {
                'marked': marked,
                'conf': max_confidence
            }
    
    # Show debug image
    st.image(cv2.cvtColor(debug_img, cv2.COLOR_BGR2RGB), caption='Bubble Detection Debug View', use_container_width=True)
    
    # Compute correctness
    final_results = {}
    correct = 0
    for q in range(1, questions+1):
        ans = answer_key.get(q, None)
        marked = results.get(q, {'marked': None, 'conf': 0})['marked']
        is_correct = (ans is not None and marked == ans)
        if is_correct:
            correct += 1
        final_results[q] = {
            'answer': ans,
            'marked': marked,
            'correct': is_correct,
            'conf': results.get(q, {'conf':0})['conf']
        }
    
    return final_results, correct
    """
    Grid-based OMR detection optimized for template with:
    - 5 subjects side by side
    - 20 questions per subject vertically
    - A,B,C,D choices for each question
    """
    # Enhanced preprocessing for better bubble detection
    # Denoise with bilateral filter
    denoised = cv2.bilateralFilter(warped_gray, 9, 75, 75)
    # Adaptive threshold for varying lighting
    thresh = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
    
    # Create debug image
    debug_img = cv2.cvtColor(warped_gray.copy(), cv2.COLOR_GRAY2BGR)
    
    # Get image dimensions
    h, w = thresh.shape
    
    # Calculate grid parameters (adjusted for header space ~20% of height)
    header_offset = int(h * 0.2)  # Skip header
    usable_h = h - header_offset
    questions_per_subject = questions // subjects  # 20 questions per subject
    subject_width = w // subjects
    row_height = usable_h // questions_per_subject
    choice_width = subject_width // choices
    
    results = {}
    
    # Process each subject
    for subject in range(subjects):
        subject_x = subject * subject_width
        
        # Process each question in this subject
        for q in range(questions_per_subject):
            question_y = header_offset + (q * row_height)
            marked = None
            max_confidence = 0
            
            # Check each choice (A,B,C,D)
            for choice in range(choices):
                x = subject_x + (choice * choice_width)
                y = question_y
                
                # Define bubble region with padding
                bubble_w = int(choice_width * 0.6)  # Reduced for precision
                bubble_h = int(row_height * 0.6)
                bubble_x = x + (choice_width - bubble_w) // 2
                bubble_y = y + (row_height - bubble_h) // 2
                
                # Ensure bounds
                bubble_x = max(0, min(bubble_x, w - bubble_w))
                bubble_y = max(0, min(bubble_y, h - bubble_h))
                
                # Check if bubble is marked
                confidence = detect_marked_bubbles(thresh, bubble_x, bubble_y, bubble_w, bubble_h)
                
                # Draw rectangle for visualization (green if marked, orange if not)
                color = (0, 255, 0) if confidence else (0, 165, 255)
                cv2.rectangle(debug_img, 
                            (bubble_x, bubble_y), 
                            (bubble_x + bubble_w, bubble_y + bubble_h),
                            color, 2)
                # Label choices
                cv2.putText(debug_img, chr(ord('A') + choice), (bubble_x, bubble_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                if confidence and confidence > max_confidence:
                    max_confidence = confidence
                    marked = chr(ord('A') + choice)
            
            # Store result for this question
            q_num = subject * questions_per_subject + (q + 1)
            results[q_num] = {
                'marked': marked,
                'conf': max_confidence
            }
    
    # Show debug image
    st.image(cv2.cvtColor(debug_img, cv2.COLOR_BGR2RGB), caption='Bubble Detection Debug View', use_container_width=True)
    
    # Compute correctness
    final_results = {}
    correct = 0
    for q in range(1, questions+1):
        ans = answer_key.get(q, None)
        marked = results.get(q, {'marked': None, 'conf': 0})['marked']
        is_correct = (ans is not None and marked == ans)
        if is_correct:
            correct += 1
        final_results[q] = {
            'answer': ans,
            'marked': marked,
            'correct': is_correct,
            'conf': results.get(q, {'conf':0})['conf']
        }
    
    return final_results, correct

# ---------- Streamlit UI ----------

st.set_page_config(page_title='OMR Evaluation System', layout='centered')

# Engaging and user-friendly color theme with CSS
st.markdown('''
    <style>
    .stApp { background-color: #F5F7FA; } /* Light pastel gray background */
    .st-bb, .stDataFrame, .stAlert, .stTextInput, .stTextArea, .stNumberInput, .stSelectbox {
        background: #FFFFFF !important;
        border-radius: 10px !important;
        box-shadow: 0 4px 12px rgba(38, 166, 154, 0.1) !important;
        color: #2F2F2F !important; /* Deep charcoal text */
    }
    h1, h2, h3, h4, h5, h6 { color: #26A69A !important; } /* Vibrant teal for headers */
    .stButton>button, .stDownloadButton>button {
        background-color: #26A69A !important; /* Teal button */
        color: #FFFFFF !important;
        border-radius: 6px !important;
        font-weight: 500;
        transition: background-color 0.3s, box-shadow 0.3s;
    }
    .stButton>button:hover, .stDownloadButton>button:hover {
        background-color: #1D7D74 !important; /* Darker teal on hover */
        box-shadow: 0 4px 16px rgba(38, 166, 154, 0.3) !important;
    }
    .stCaption, .stInfo, .stSuccess, .stWarning, .stError, .stAlert, .stMarkdown, .stText, .stDataFrame, .stTable, .stTextInput, .stTextArea, .stNumberInput, .stSelectbox {
        color: #666666 !important; /* Muted gray for secondary text */
    }
    .stMarkdown p, .stText, .stDataFrame, .stTable {
        color: #2F2F2F !important; /* Deep charcoal for body text */
    }
    /* Enhanced File Uploader */
    .stFileUploader {
        background: #E0F2F1 !important; /* Light teal background */
        border-radius: 16px !important;
        box-shadow: 0 4px 16px rgba(38, 166, 154, 0.2) !important;
        border: 2px dashed #26A69A !important; /* Teal dashed border */
        padding: 1.2em 1em 1.2em 1em !important;
        color: #2F2F2F !important;
        margin-bottom: 1.5em !important;
    }
    .stFileUploader label, .stFileUploader div, .stFileUploader span {
        background: transparent !important;
        color: #2F2F2F !important;
    }
    .stFileUploader button {
        background-color: #26A69A !important;
        color: #FFFFFF !important;
        font-weight: 600;
        border-radius: 8px !important;
        box-shadow: 0 2px 8px rgba(38, 166, 154, 0.3) !important;
        border: none !important;
        padding: 0.5em 1.2em !important;
        margin-top: 0.5em !important;
        transition: background-color 0.3s, box-shadow 0.3s;
    }
    .stFileUploader button:hover {
        background-color: #1D7D74 !important;
        box-shadow: 0 4px 16px rgba(38, 166, 154, 0.4) !important;
    }
    /* Remove unwanted default styles */
    * { color: #2F2F2F !important; background: none !important; }
    </style>
''', unsafe_allow_html=True)

st.title('OMR Evaluation & Scoring System')
st.markdown('''
<div style="font-size:1.1em; color:#2F2F2F; margin-bottom:1em;">
<b>How to use:</b><br>
1. <b>Upload your answer key</b> (CSV, Excel, or JSON) or paste answers.<br>
2. <b>Set the number of questions, choices, and subjects</b> if needed.<br>
3. <b>Upload OMR answer sheet images</b> (JPG/PNG).<br>
4. <b>View results instantly below.</b>
</div>
''', unsafe_allow_html=True)

st.header('Step 1: Upload Answer Key')
key_file = st.file_uploader('Upload answer key (CSV, Excel, or JSON)', type=['csv','json','xlsx','xls'])
manual_key = st.text_area('Or paste answers as comma-separated values (A,B,C,...)', value='')
answer_key = {}
if key_file is not None:
    answer_key = read_answer_key(key_file)
    st.success(f'Loaded {len(answer_key)} answers from file')
elif manual_key.strip():
    answers = [x.strip().upper() for x in manual_key.split(',') if x.strip()]
    answer_key = {i+1: answers[i] for i in range(len(answers))}
    st.success(f'Loaded {len(answer_key)} answers from manual input')
else:
    st.info('Please upload or paste an answer key.')

st.header('Step 2: Set Options')
col1, col2, col3 = st.columns(3)
with col1:
    questions = st.number_input('Total questions', min_value=1, max_value=500, value=100)
with col2:
    choices = st.selectbox('Choices per question', options=[2,3,4,5], index=2)
with col3:
    subjects = st.number_input('Subjects/sections', min_value=1, max_value=10, value=5)
st.caption('Tip: Adjust these if your OMR template is different.')

st.header('Step 3: Upload OMR Sheet Images')
uploaded = st.file_uploader('Upload OMR answer sheet images (JPG/PNG)', type=['png','jpg','jpeg'], accept_multiple_files=True)

if uploaded and answer_key:
    for file in uploaded:
        st.subheader(f'Processing: {file.name}')
        image = Image.open(file).convert('RGB')
        img_np = np.array(image)
        orig = img_np.copy()
        # resize for speed but keep aspect ratio
        img_resized = resize(img_np, width=1200)
        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)

        paper = find_paper_contour(gray)
        if paper is not None:
            warped = four_point_transform(img_resized, paper)
            warped_gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)
        else:
            st.warning('Could not detect sheet border â€” trying to process the full image as sheet.')
            warped = img_resized
            warped_gray = gray

        # show warped preview
        st.image(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB), caption='Detected / Warped Sheet Preview', use_container_width=True)

        # grade (pass subjects) - using grid-based detection for precise recognition
        results, correct = grade_sheet(warped_gray, answer_key, questions=int(questions), choices=int(choices), subjects=int(subjects))
        score = correct
        st.write(f'**Total Correct:** {correct} / {questions}   |   **Score (raw):** {score}')

        # show per-question summary (paginated small table)
        df = pd.DataFrame.from_dict(results, orient='index')
        df.index.name = 'question'
        st.dataframe(df)

        # Optional per-subject aggregation
        st.info('If you want per-subject scores, upload a mapping file (CSV) with columns: question,subject')
        subj_map_file = st.file_uploader(f'Optional: Upload question->subject mapping for {file.name}', type=['csv','json'], key=file.name + '_map')
        if subj_map_file is not None:
            try:
                sm_df = pd.read_csv(subj_map_file) if subj_map_file.name.endswith('.csv') else pd.read_json(subj_map_file)
                mapping = {int(r['question']): r['subject'] for _, r in sm_df.iterrows()}
                df['subject'] = df.index.map(lambda q: mapping.get(int(q), 'Unknown'))
                agg = df.groupby('subject')['correct'].sum().reset_index()
                st.write('Per-subject scores:')
                st.table(agg)
            except Exception as e:
                st.error('Could not parse subject map file. Ensure columns question and subject exist.')

        # Optionally allow download of detailed result as CSV
        to_download = df.reset_index().to_csv(index=False).encode('utf-8')
        st.download_button('Download detailed results CSV', data=to_download, file_name=f'results_{file.name}.csv', mime='text/csv')

elif uploaded and not answer_key:
    st.warning('Please upload an answer key first (left panel).')

else:
    st.info('Upload answer key and OMR image(s) to begin grading.')

st.markdown('---')
st.write('Template tuning tips:')
st.write('- If the bubble detection misses marks, adjust the threshold in detect_marked_bubbles or grid parameters in grade_sheet.')
st.write('- Debugging: Use the Bubble Detection Debug image to tune parameters.')

# EOF
